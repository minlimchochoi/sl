---
layout: page-sidenav
group: "VAE"
title: "1. Introduction"
---


- [VAE (Kingma & Welling, 2014)](https://arxiv.org/abs/1312.6114) 논문 요약 설명
- [Auto-Encoder](https://en.wikipedia.org/wiki/Autoencoder) 기반 [Variational Bayes](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) 추론을 설명한다
- [Change of Variable](https://en.wikipedia.org/wiki/Integration_by_substitution) 을 이용한 Reparametrization trick 을 이해한다

### 미리 알아두면 좋은 용어들

- [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
- [Change of Variable](https://en.wikipedia.org/wiki/Integration_by_substitution)
- [Auto-Encoder](https://en.wikipedia.org/wiki/Autoencoder)
- [Variational Inference](http://norman3.github.io/prml/docs/chapter10/1)


---
## What is VAE?

- VAE 

---

## ELBO 

- ELBO (**E**vidence **L**ower **Bo**und) 는 VAE 에서의 objective function 이다

$$
\mathcal{L}_{\theta,\phi}(x)=\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z|x)\right]
$$

- ELBO 라는 이름은 evidence (또는 marginal likelihood) $$ \log p_{\theta}(x) $$ 의 **하한(lower bound)** 이기 때문에 주어진 이름이다

$$
\mathcal{L}_{\theta,\phi}(x)=\log p_{\theta}(x)-\mathbb{KL}(q_{\phi}(z|x)\Vert p_{\theta}(z|x)) \leq \log p_{\theta}(x)
$$

- ELBO 를 optimize 하려면 두 개의 gradient 를 구해야한다:

	- GM 의 parameter $$ \theta $$ 에 대한 gradient $$ \nabla_{\theta} $$ 
	- Variational parameter $$ \phi $$ 에 대한 gradient $$ \nabla_{\phi} $$ 

- 첫번째 gradient $$ \nabla_{\theta} $$ 를 구하는 건 어렵지 않다. 기대값 계산에 사용되는 확률밀도함수 $$ q_{\phi} $$ 가 $$ \theta $$ 랑 상관없는 함수이므로 그냥 기대값에 집어넣으면 된다. 이 때 $$ -\log q_{\phi}(z|x) $$ 도 같이 날라간다 :

$$
\begin{aligned}
\nabla_{\theta} \mathcal{L}_{\theta,\phi}(x)  &= \nabla_{\theta}\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z|x)\right] \\
& = \mathbb{E}_{q_{\phi}(z|x)}\left[\nabla_{\theta}\log p_{\theta}(x,z)\right]
\end{aligned}
$$

- 문제는 (항상) 두번째다. 이번에는 $$ \nabla_{\phi} $$ 가 아예 $$ \phi $$ 를 미분하는 연산자이므로 $$ q_{\phi} $$ 도 같이 미분해줘야 한다. 그러므로 기대값에 미분연산을 집어넣으면 큰일난다 :

$$
\begin{aligned}
\nabla_{\phi} \mathcal{L}_{\theta,\phi}(x)  &= \nabla_{\phi}\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z|x)\right] \\
& \neq \mathbb{E}_{q_{\phi}(z|x)}\left[\nabla_{\phi}\log p_{\theta}(x,z)-\nabla_{\phi}\log q_{\phi}(z|x)\right]
\end{aligned}
$$

- 이걸 해결하기 위해 등장한 방법이 바로 Reparametrization trick 이다


---

## Reparametrization Trick

- Reparametrization trick 은 latent variable $$ z $$ 의 randomness 를 결정하는 random variable $$ \epsilon $$ 을 도입하는 것이다 :

$$ z = g(\epsilon,\phi,x) $$ 

![figure1.1]({{ site.baseurl }}/images/agm_Figure020101.png){:class="center-block" height="200px"}

- 가령 $$ z \sim \mathcal{N}(\mu, \sigma^{2}) $$ 를 따른다고 하자. 이 때의 variational parameter 는 $$ \phi = (\mu, \sigma) $$ 이다. 그런데 $$ z $$ 는 다음과 같이 표현할 수 있다

$$
z \overset{d}{=} \mu + \sigma \epsilon,\quad \epsilon\sim\mathcal{N}(0,1)
$$

- 위 식에서 값이 같다는 의미에서 $$ = $$ 가 아니라 **분포가 같다** 는 의미인 $$ \overset{d}{=} $$ 으로 표현했다는 사실을 기억하자. Reparametrization trick 은 $$ z $$ 와 분포가 같은 다른 확률변수를 찾는 방법이다.

---
#

### 참고자료

