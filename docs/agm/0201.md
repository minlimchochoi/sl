---
layout: page-sidenav
group: "VAE"
title: "1. Introduction"
---


- [VAE (Kingma & Welling, 2014)](https://arxiv.org/abs/1312.6114) 논문 요약 설명
- [Auto-Encoder](https://en.wikipedia.org/wiki/Autoencoder) 기반 [Variational Bayes](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) 추론을 설명한다
- [Change of Variable](https://en.wikipedia.org/wiki/Integration_by_substitution) 을 이용한 Reparametrization trick 을 이해한다

### 미리 알아두면 좋은 용어들

- [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
- [Change of Variable](https://en.wikipedia.org/wiki/Integration_by_substitution)
- [Auto-Encoder](https://en.wikipedia.org/wiki/Autoencoder)
- [Variational Inference](http://norman3.github.io/prml/docs/chapter10/1)


---
## What is VAE?

- VAE 

---

## ELBO 

- ELBO (**E**vidence **L**ower **Bo**und) 는 VAE 에서의 objective function 이다

$$
\mathcal{L}_{\theta,\phi}(x)=\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z \vert x)\right]
$$

- ELBO 라는 이름은 evidence (또는 marginal likelihood) $$ \log p_{\theta}(x) $$ 의 **하한(lower bound)** 이기 때문에 주어진 이름이다

$$
\mathcal{L}_{\theta,\phi}(x)=\log p_{\theta}(x)-\mathbb{KL}(q_{\phi}(z \vert x)\Vert p_{\theta}(z \vert x)) \leq \log p_{\theta}(x)
$$

- ELBO 를 optimize 하려면 두 개의 gradient 를 구해야한다:

	- GM 의 parameter $$ \theta $$ 에 대한 gradient $$ \nabla_{\theta} $$ 
	- Variational parameter $$ \phi $$ 에 대한 gradient $$ \nabla_{\phi} $$ 

- 첫번째 gradient $$ \nabla_{\theta} $$ 를 구하는 건 어렵지 않다. 기대값 계산에 사용되는 확률밀도함수 $$ q_{\phi} $$ 가 $$ \theta $$ 랑 상관없는 함수이므로 그냥 기대값에 집어넣으면 된다. 이 때 $$ -\log q_{\phi}(z \vert x) $$ 도 같이 날라간다 :

$$
\begin{aligned}
\nabla_{\theta} \mathcal{L}_{\theta,\phi}(x)  &= \nabla_{\theta}\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z|x)\right] \\
& = \mathbb{E}_{q_{\phi}(z|x)}\left[\nabla_{\theta}\log p_{\theta}(x,z)\right]
\end{aligned}
$$

- 문제는 (항상) 두번째다. 이번에는 $$ \nabla_{\phi} $$ 가 아예 $$ \phi $$ 를 미분하는 연산자이므로 $$ q_{\phi} $$ 도 같이 미분해줘야 한다. 그러므로 기대값에 미분연산을 집어넣으면 큰일난다 :

$$
\begin{aligned}
\nabla_{\phi} \mathcal{L}_{\theta,\phi}(x)  &= \nabla_{\phi}\mathbb{E}_{q_{\phi}(z \vert x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z \vert x)\right] \\
& \neq \mathbb{E}_{q_{\phi}(z \vert x)}\left[\nabla_{\phi}\log p_{\theta}(x,z)-\nabla_{\phi}\log q_{\phi}(z \vert x)\right]
\end{aligned}
$$

- 이걸 해결하기 위해 등장한 방법이 바로 지금부터 소개할 Reparametrization trick 이다




## Reparametrization Trick

![figure1.1]({{ site.baseurl }}/images/agm_Figure020101.png){:class="center-block" height="300px"}

- Reparametrization trick 은 latent variable $$ z $$ 의 randomness 를 결정하는 random variable $$ \epsilon $$ 을 도입하는 것이다 :

$$ z = g(\phi,x,\epsilon) $$ 

- 가령 $$ z \sim \mathcal{N}(\mu, \sigma^{2}) $$ 를 따른다고 하자. 이 때의 variational parameter 는 $$ \phi = (\mu, \sigma) $$ 이다. 이 경우 $$ z $$ 는 다음과 같이 표현할 수 있다

$$
z \overset{d}{=} \mu + \sigma \epsilon,\quad \epsilon\sim\mathcal{N}(0,1)
$$

- 위 식에서 값이 같다는 의미에서 $$ = $$ 가 아니라 **분포가 같다** 는 의미인 $$ \overset{d}{=} $$ 으로 표현했다는 사실을 기억하자. Reparametrization trick 은 $$ z $$ 와 분포는 같지만 다른 확률변수 $$ \mu + \sigma \epsilon $$ 을 찾아서 기대값을 대신 계산하는 방법이다! $$ z $$ 는 어짜피 latent variable 이므로 ELBO 처럼 기대값을 계산할 때는 분포가 같은 확률변수를 찾는 것으로도 충분한 것이다

- 따라서 ELBO 의 기대값 계산은 $$ q_{\phi}(z\vert x) $$ 대신 $$ p(\epsilon) $$ 을 이용하여 사용할 수 있다. 이런 변환이 가능한 이유가 바로 change of variable formula 이다 :

$$
\begin{aligned}
\mathcal{L}_{\theta,\phi}(x) &= \mathbb{E}_{q_{\phi}(z\vert x)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z\vert x)\right] \\
&= \int\left[\log p_{\theta}(x,z)-\log q_{\phi}(z\vert x)\right] q_{\phi}(z\vert x) dz \\
&= \int_{\Omega} \left[\log p_{\theta}(x,Z(\omega))-\log q_{\phi}(Z(\omega) \vert x) \right] \mathbb{P}(d\omega )\\
&= \int \left[p_{\theta}(x,g(\phi,x,\epsilon))-\log q_{\phi}(g(\phi,x,\epsilon)\vert x)\right] p(\epsilon) d\epsilon \\
&= \mathbb{E}_{p(\epsilon)}\left[\log p_{\theta}(x,z)-\log q_{\phi}(z \vert x)\right]
\end{aligned}
$$



- 위에서 적분변수가 $$ \mathbb{P}(d\omega) $$ 에서 $$ d\epsilon $$ 으로 변하는게 바로 measure theory 버젼의 change of varialbe formula 이다
	- 보통 학부 확률론이나 미적분학에서는 여기에 Jacobian matrix 가 등장하고 함수 $$ g $$ 의 invertibility 를 요구한다. Kingma 의 논문에서도 이 점을 언급하고 있는데, 본래 저 조건은 확률밀도함수를 명시적으로(explicitly) 쓰기 위해 필요한 것이지 변수변환 자체의 필요조건은 아니다
	- 물론 실제로 컴퓨터를 통해 계산할 때는 연산이 쉬운 확률밀도함수가 필요하므로 $$ g $$ 를 굉장히 좋은 함수로 가정할 것이다

- 어쨋거나 ELBO 의 확률밀도함수를 $$ \phi $$ 에 상관없는 함수로 바꾸었으므로 이제 문제였던 두 번째 gradient $$ \nabla_{\phi} $$ 가 기대값 안으로 들어갈 수 있다!

$$
\nabla_{\phi} \mathcal{L}_{\theta,\phi}(x) = \mathbb{E}_{p_{\epsilon}(\epsilon)}\left[\nabla_{\phi}\log p_{\theta}(x,z)-\nabla_{\phi}\log q_{\phi}(z \vert x)\right]
$$


---
#

### 참고자료

