---
layout: post
title: "Generalization in Deep Learning"
---

2017, 논문링크 : [https://arxiv.org/abs/1710.05468](https://arxiv.org/abs/1710.05468)

---

- 저자 : Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio

- 초록 (Abstract)

```text
This paper explains why deep learning can generalize well, despite large capacity and possible algorithmic instability, nonrobustness, and sharp minima, effectively addressing an open problem in the literature. Based on our theoretical insight, this paper also proposes a family of new regularization methods. Its simplest member was empirically shown to improve base models and achieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. Moreover, this paper presents both data-dependent and data-independent generalization guarantees with improved convergence rates. Our results suggest several new open areas of research.
```

- 요약 (Summary)
	- 딥러닝의 일반화(Generalization) 성능에 대해 [Zhang et al.](https://arxiv.org/abs/1611.03530) 등은 
	- 


- 필요한 사전지식 (Preliminaries)
	- [Zhang et al. 논문 (ICLR, 2017)](https://arxiv.org/abs/1611.03530)
	- Radamecher random variable
	- 

- 기호 (Notation)
	- Input space : $$ x\in\mathcal{X} $$ 
	- Target space : $$ y\in\mathcal{Y} $$
	- Size of a training set : $$ m $$
	- Learning Algorithm : $$ \mathcal{A} $$
	- Training dataset : $$ S_{m} = \{ (x_{1},y_{1}),\ldots, (x_{m},y_{m}) \} $$
		- $$ S_{m} $$ 의 각 원소들은 i.i.d 샘플링 되었다고 가정
		- 
    - Model learned by $$ \mathcal{A} $$ with $$ S_{m} $$ : $$ f_{\mathcal{A}(S_{m})} :\mathcal{X} \to \mathcal{Y} $$ 
	- Loss function : $$ \ell $$  
	- Expected risk : $$ R[f] = \mathbb{E}_{x,y \sim P_{(x,y)} }[\ell(f(x),y)] $$
	- Empirical risk : $$ \hat{R}[f] = \frac{1}{m}\sum_{i=1}^{m}[\ell(f(x_{i}),y_{i})] $$ where $$ (x_{i}, y_{i})\sim P_{(x,y)} $$
	- Hypothesis set (or model class) : $$ \mathcal{F} $$
	- Family of loss function : $$ \mathcal{L}_{\mathcal{F}} = \{ \ell(f(\cdot),\cdot) : f\in\mathcal{F} \} $$
	- Trainable parameters : $$ w\in\mathbb{R}^{n} $$
	- Pre-activation vector : $$ h^{(l)}(x) $$
	- Number of hidden layers : $$ H $$
	- Largest singular value of matrix $$ M $$ : $$ \lambda_{\max }(M) $$
	- Spectral norm of matrix $$ M $$ : $$ \Vert M \Vert $$


---


![figure1.1]({{ site.baseurl }}/images/posts/2017-09-30-Figure1.png){:class="center-block" height="200px"}

---

`Theorem 3.4` Assumption 3.2 하에서, 학습데이터들이 선형적으로 독립(linearly independent) 이라고 하자. 이 때 식 (1) 의 모든 critical point \\( (W_{l},b_{l})_{l=1}^{L} \\) 들은 Weight 행렬들이 full-column rank 를 가질 때 global minimum 이다


---

