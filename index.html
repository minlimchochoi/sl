---
layout: default
---
<div class="row">
  <div class="col-sm-12 col-md-6">
    <div class="well-sm">
      <h3>All of <span style="color:blue">S</span>tatistical <span style="color:blue">L</span>earning</h3>
      <p> by <span style="color:blue">S</span>ungbin <span style="color:blue">L</span>im </p>
      <p>
 	      <img src="profile.jpg" width=150 height=150>
      </p>
      <p>
 	      <a href="https://www.dropbox.com/s/huik76zchc9n5dl/CV_sungbin.pdf?dl=0" target="_blank">
	        <button type="button" class="btn btn-success">CV</button>
	      </a>
      </p>
    </div>
    
    <div class="well-sm">
      <h3>New Posts</h3>
      <ul class="post-list">
	      {% for post in site.posts limit:3 %}
	      <li>
	        <h4>
            <a href="{{ post.url | prepend: site.baseurl }}">{{ post.title }}</a>
	        </h4>
	        <p><span class="post-date">{{ post.date | date: "%b %-d, %Y" }}</span></p>
	        {{ post.excerpt }}
          <hr/>          
	      </li>
	      {% endfor %}
      </ul>
    </div>
  </div>
  
  <div class="col-sm-12 col-md-6">
    <div class="well-sm">
      <h3>Breaking Papers</h3>
      <ul>
        <li><a href="{{ site.baseurl }}/docs/papers/0"> GAN : Generative Adversarial Nets</a></li>
        <li><a href="{{ site.baseurl }}/docs/papers/1"> VAE : Variational Auto-Encoder</a></li>
      </ul>
    </div>
    <div class="well-sm">
      <h3>1. All of Statistics</h3>
      <ul>
        <li><a href="{{ site.baseurl }}/docs/chapter01/0"> Part 1. Probability Theory </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter01/1"> Part 2. Statistical Inference </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter01/2"> Part 3. Statistical Models and Methods </a></li>
      </ul>
    </div>
    <div class="well-sm">
      <h3>2. All of Nonparametric Statistics</h3>
      <ul>
        <li><a href="{{ site.baseurl }}/docs/chapter02/0"> 1. Introduction </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/1"> 2. Estimating the CDF and Statistical Functionals </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/2"> 3. The Bootstrap and the Jacknife</a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/3"> 4. Smoothing: General Concepts </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/3"> 5. Nonparametric Regression </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/3"> 6. Density Estimation </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/3"> 7. Normal Means and Minimax Theory </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/3"> 8. Nonparametric Inference Using Orthogonal Functions </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/3"> 9. Wavelets and Other Adaptive Methods </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter02/3"> 10 Other Topics</a></li>
      </ul>
    </div>
    <div class="well-sm">
      <h3>3. Elements of Statistical Learning</h3>
      <ul>
        <li><a href="{{ site.baseurl }}/docs/chapter03/0"> 1. Introduction</a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/1"> 2. Overview of Supervised Learning </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/2"> 3. Linear Methods for Regression </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/3"> 4. Linear Methods for Classification </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/4"> 5. Basic Expansions and Regularization </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/5"> 6. Kernel Smoothing Methods </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/6"> 7. Model Assessment and Selection </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 8. Model Inference and Averaging </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 9. Additive Models, Trees, and Related Methods </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 10. Boosting and Additive Trees </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 11. Neural Networks </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 12. Support Vector Machines and Flexible Discriminants </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 13. Prototype Methods and Nearest-Neighbors </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 14. Unsupervised Learning </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 15. Random Forests </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 16. Ensemble Learning </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 17. Undirected Graphical Models </a></li>
	<li><a href="{{ site.baseurl }}/docs/chapter03/7"> 18. High-Dimensional Problems </a></li>
      </ul>
    </div>

    <div class="well-sm">
      <h3>4. Modern Probability Theory</h3>
      <ul>
        <li><a href="{{ site.baseurl }}/docs/chapter04/0">0. Kolmogorov Probability Axiom </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter04/1">1. Basic Measure Theory </a></li>
        <li><a href="{{ site.baseurl }}/docs/chapter04/2">2. Gaussian Processes </li>
        <li><a href="{{ site.baseurl }}/docs/chapter04/3">3. Markov Theory </li>
        <li><a href="{{ site.baseurl }}/docs/chapter04/4">4. Stochastic Calculus </li> 
      </ul>
    </div>



    
  </div>
</div>



